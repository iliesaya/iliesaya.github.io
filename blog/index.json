[
    {
        "ref": "http://iliesaya.github.io/blog/backup_jetson/",
        "title": "Full backup Nvidia Jetson Xavier",
        "section": "blog",
        "date" : "2019.09.15",
        "body": " Command to do a full backup of the NVIDIA jetson xavier source is from here\nCreating the backup image file You will need a PC running ubuntu with Nvidia sdk JetPack 4.2 and 64go of free space. From this PC, ssh to your jetson (jetson is running normaly, not on restore mode).\nssh jetsonUser@JetsonIP  On the Jetson, through this ssh, stop the filesystem and force it to read only access:\necho u \u0026gt; /proc/sysrq-trigger  Still on the jetson, through ssh, transferring an image of full internal memory hard drive over ssh to host PC:\ndd if=/dev/mmcblk0p1 | ssh user@hostpc dd of=/media/aya/usbaya/image.raw  I am creating the raw img file in my external drive for space issue. I\nThis will create on your PC a 30Gb file containing a full image of the Jetson. you can now turn of jetson.\nNow on you pc you can convert the .raw image to a .img image file:\ncd /home/aya/nvidia/nvidia_sdk/JetPack_4.2_Linux_P2888/Linux_for_Tegra/bootloader/ sudo ./mksparse -v --fillpattern=0 /media/aya/usbaya/image.raw /media/aya/usbaya/system.img  I am creating the img file in my external drive for space issue. I\nRestoring the image file Boot the Jetson on restore mode (power and middle button for 5 sec), connect it with usb to the PC.\ncd /home/aya/nvidia/nvidia_sdk/JetPack_4.2_Linux_P2888/Linux_for_Tegra/bootloader/  copy in this bootloader folder your backup img file and rename it system.img\nsudo ./flash.sh -r jetson-xavier mmcblk0p1  done.\n"
    }
,
    {
        "ref": "http://iliesaya.github.io/blog/junctionxkaust/",
        "title": "JunctionX KAUST 2019 : Predicting the Future of Solar Power Generation at NEOM",
        "section": "blog",
        "date" : "2019.09.15",
        "body": " Solar power generation forecasts will be a critical need if SOs are to balance NEOM\u0026rsquo;s grid. We built an ML model to forecast solar power generation that takes into account NEOM\u0026rsquo;s weather patterns. Devpost link\nInspiration Solar power generation forecasts will be a critical need if SOs are to balance NEOM\u0026rsquo;s smart electricity grid with nearly 100% renewables. Even though NEOM is blessed with plenty of solar radiation, NEOM also experiences substantial fluctuations in temperature, wind, and dust and these factors can all have a substantial impact on solar power generation. We built an ML model capable of accurately forecasting solar power generation that takes into account NEOM\u0026rsquo;s unique weather patterns and created a few prototype interactive dashboards to display the data.\nWhat it does Our model uses ML techniques to predict future solar power generation at NEOM as a function of the weather data as well as the history solar power generation. After training our model on historical data, we can generate a new forecast for next day\u0026rsquo;s solar power generation. Once the next day\u0026rsquo;s actual values of solar power generation are observed, our model can be automatically re-trained and improved. Model can easily be retrained with weekly or monthly forecast horizons if longer forecasts are required by the SO.\nOur interactive dashboard allows the user to interrogate the historical weather and solar power generation data for NEOM as well as to display forecasts of future solar power generation. Dashboard can generate user notifications via the Telegram mobile app to indicate significantly changes to either weather or solar energy forecasts.\nOur web and mobile app prototype will allow users (SOs, residential, and industrial prosumers) at NEOM to interact with weather and solar energy forecasts and to receive alerts about significant upcoming changes to either.\nHow we built it We built the ML model using widely used open-source tools: Python, Jupyter, Scikit-learn, Keras, and Tensorflow. Our interactive dashboard leveraged another open-source tool called Grafana. We used Adobe XD to prototype our web and mobile apps.\nChallenges we ran into  Finding the right machine learning approach. We evaluated a number of classical ML approaches including linear regression, multi-task elasticnet, multi-task lass regression, random forest regression, and GRU and LSTM deep neural networks. Random forest regression eventually emerged as the most performant.\n Finding the right dashboard tool. We attempted to roll our own but then decided this approach was too time consuming and explored a few off-the-shelf dashboard solutions before eventually settling on Grafana.\n Injecting the ML model into a web application turned out to be significantly more difficult than we had suspected.\n  Accomplishments that we\u0026rsquo;re proud of  Our overall concept was very solid and useful. Our interactive dashboard with mobile notifications is really cool! Our ML modeling pipeline with good forecasting results could be put into production at NEOM.  What we learned  New machine learning approached and data analysis techniques. Serving machine learning models in web and mobile apps is hard! Teamwork!  What\u0026rsquo;s next for Forecasting Solar Power Generation for NEOM using ML? Predicting the future or electricity demand at NEOM!\nBuilt With  python keras react node.js tensorflow scikit-learn jupyter grafana pandas  Try it out  GitHub Repo GitHub Repo (fork on mine)  mybinder.org   "
    }
,
    {
        "ref": "http://iliesaya.github.io/blog/tensorflow_riaps_jetson/",
        "title": "Tensorflow not initializing during riaps deployment",
        "section": "blog",
        "date" : "2019.09.08",
        "body": " Strange error cannot allocate memory in static TLS block under riaps plateform I am deploying on an nvidia Jetson Xavier a Tensorflow / Keras application (tensorflow-gpu==1.13.1+nv19.5).\nIf I launch my python code outside riaps, it\u0026rsquo;s working , no error. But when I deploy it with riaps_ctrl and launch it, here is the result :\n \u0026lt;class 'ImportError'\u0026gt;: Traceback (most recent call last): File \u0026quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\u0026quot;, line 58, in \u0026lt;module\u0026gt; from tensorflow.python.pywrap_tensorflow_internal import * File \u0026quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u0026quot;, line 28, in \u0026lt;module\u0026gt; _pywrap_tensorflow_internal = swig_import_helper() File \u0026quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u0026quot;, line 24, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File \u0026quot;/usr/lib/python3.6/imp.py\u0026quot;, line 243, in load_module return load_dynamic(name, filename, file) File \u0026quot;/usr/lib/python3.6/imp.py\u0026quot;, line 343, in load_dynamic return _load(spec) ImportError: /usr/lib/aarch64-linux-gnu/libgomp.so.1: cannot allocate memory in static TLS block Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common reasons and solutions. Include the entire stack trace above this error message when asking for help. Traceback (most recent call last): File \u0026quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\u0026quot;, line 58, in \u0026lt;module\u0026gt; from tensorflow.python.pywrap_tensorflow_internal import * File \u0026quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u0026quot;, line 28, in \u0026lt;module\u0026gt; _pywrap_tensorflow_internal = swig_import_helper() File \u0026quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u0026quot;, line 24, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File \u0026quot;/usr/lib/python3.6/imp.py\u0026quot;, line 243, in load_module return load_dynamic(name, filename, file) File \u0026quot;/usr/lib/python3.6/imp.py\u0026quot;, line 343, in load_dynamic return _load(spec) ImportError: /usr/lib/aarch64-linux-gnu/libgomp.so.1: cannot allocate memory in static TLS block  Impossible to update the jetson tensorflow version, because cudnn installed version is 7.3.1 and I don\u0026rsquo;t want to re-flash the device and lose all my riaps installation. If I use a more recent version of tensorflow I have this error:\nLoaded runtime CuDNN library: 7.3.1 but source was compiled with: 7.5.0  The solution I found is to move the Tensorflow include at the very top of the python code :\nimport tensorflow as tf from tensorflow import keras from tensorflow.keras.layers import GlobalAveragePooling2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Input, Dropout, Conv2D, BatchNormalization, MaxPool2D, Activation from tensorflow.keras.models import Model, Sequential from tensorflow.keras.optimizers import Adam from tensorflow.keras.regularizers import l2 from tensorflow.keras.layers import MaxPooling1D, Conv1D, GlobalAveragePooling1D, Reshape from riaps.run.comp import Component import logging from random import random import time as t from time import mktime from datetime import datetime import numpy as np from sklearn import preprocessing from sklearn.preprocessing import normalize  If the import tensorflow are after the sklearn import, it\u0026rsquo;s not working. This page gave me the hint : https://www.mail-archive.com/python-devel@lists.fedoraproject.org/msg01516.html\nI think this bug is specific to the version of tensorflow I have on the jetson because the raspberry pi doesn\u0026rsquo;t have this issue.\n"
    }
]
